{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST DATASET\n",
    "#### What is the MNIST dataset?\n",
    "\n",
    "The MNIST dataset or Modified National Institute of Standards and Technology dataset is a large collection of 60000 training images, and 10000 testing images. Half of the collection comprises of handwritten digits from the original NIST's training dataset and the other half is from its testing dataset. The NIST training dataset was taken from the American Census Bureau employees and the testing set was taken from American high school students, so the discrepancies between the two handwriting styles and quality was quite different. This is why the MNIST dataset is a mix of the two. Also the images were normalized, and centered to fit to a 28x28 pixel, which was also anti-aliased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "For Keras, loading the dataset, saving the model, preprocessing and displaying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Seeding the model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "from keras import backend as K \n",
    "import os\n",
    "\n",
    "# Training and reading the dataset\n",
    "import keras as kr\n",
    "from keras.models import load_model # To save and load models.\n",
    "from keras.datasets import mnist # Import the MNIST dataset directly from Keras, which is faster.\n",
    "\n",
    "import sklearn.preprocessing as pre # For encoding categorical variables.\n",
    "import sklearn.decomposition as dec # For Principal component analysis (PCA) & Whitening\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # Plot results graphically.\n",
    "import gzip # \n",
    "import pandas as pd # For displaying data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the MNIST dataset\n",
    "To learn more about how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Read in the training and test images from gzip file (Slow)\n",
    "\n",
    "* From testing this is quite slow compared to reading directly from keras.datasets which is used below, but it good for testing and learning how the dataset is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://docs.python.org/3/library/gzip.html\n",
    "# Open the MNIST dataset of 10000 testing images.\n",
    "with gzip.open('MNIST_Images/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    test_imgs = f.read()\n",
    "\n",
    "# Open the label dataset like before.\n",
    "with gzip.open('MNIST_Images/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    test_lbls = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in bytes from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Type: <class 'bytes'>\n",
      "Bytes: b'\\x00\\x00\\x08\\x03'\n"
     ]
    }
   ],
   "source": [
    "# Print the file type of the dataset - bytes\n",
    "print(\"File Type:\", type(test_imgs))\n",
    "\n",
    "# Print out the first four bytes.\n",
    "# b'\\x00\\x00\\x08\\x03' is the output which correlates with the MNIST website results if reading from a gzip.\n",
    "print(\"Bytes:\", test_imgs[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big-endian: 2051\n",
      "Little-endian: 50855936\n"
     ]
    }
   ],
   "source": [
    "# Adapted from: https://stackoverflow.com/questions/51220161/how-to-convert-from-bytes-to-int\n",
    "# Convert bytes to a 32 bit integer using big-endian and little-endian.\n",
    "print(\"Big-endian:\", int.from_bytes(test_imgs[0:4], byteorder='big')) #2051\n",
    "print(\"Little-endian:\", int.from_bytes(test_imgs[0:4], byteorder='little')) #50855936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display single image from dataset and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAANYUlEQVR4nO3df6hc9ZnH8c9n3QTEFk0ihouRtUaF1UWtXGXRsrjURlc0MWDXBFlcVrj9o0LF+CNkhQiLKLvb3T8DtzQ0atemITGNtWwqof5YMMGrxJg0aTUS0zTXXLIBmyBSkzz7xz13uU3unLk5Z2bOJM/7BZeZOc/M9zyMfnLOzJlzvo4IATj3/VnTDQDoDcIOJEHYgSQIO5AEYQeS+PNersw2X/0DXRYRnmp5rS277Ttt/8b2R7aX1xkLQHe56nF22+dJ+q2kb0k6IOkdSUsj4tclr2HLDnRZN7bsN0v6KCI+jog/SvqJpEU1xgPQRXXCfqmk3016fKBY9idsD9kesT1SY10AaqrzBd1Uuwqn7aZHxLCkYYndeKBJdbbsByRdNunxPEkH67UDoFvqhP0dSVfZ/prtmZKWSNrUmbYAdFrl3fiIOG77YUmbJZ0naXVE7OpYZwA6qvKht0or4zM70HVd+VENgLMHYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSfT0UtKo5rHHHiutn3/++S1r1113Xelr77vvvko9TVi1alVp/e23325Ze+GFF2qtG2eGLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMHVZfvA2rVrS+t1j4U3ae/evS1rt99+e+lr9+/f3+l2UuDqskByhB1IgrADSRB2IAnCDiRB2IEkCDuQBOez90CTx9H37NlTWt+8eXNp/Yorriit33PPPaX1+fPnt6w98MADpa999tlnS+s4M7XCbnufpKOSTkg6HhGDnWgKQOd1Ysv+txFxuAPjAOgiPrMDSdQNe0j6pe13bQ9N9QTbQ7ZHbI/UXBeAGuruxt8aEQdtXyLpNdt7IuLNyU+IiGFJwxInwgBNqrVlj4iDxe2YpJcl3dyJpgB0XuWw277A9lcn7ktaIGlnpxoD0Fl1duPnSnrZ9sQ4/xUR/92Rrs4yg4PlRxwXL15ca/xdu3aV1hcuXNiydvhw+YGSY8eOldZnzpxZWt+6dWtp/frrr29ZmzNnTulr0VmVwx4RH0tq/V8SQF/h0BuQBGEHkiDsQBKEHUiCsANJcIprBwwMDJTWi8OTLbU7tHbHHXeU1kdHR0vrdSxbtqy0fs0111Qe+9VXX638Wpw5tuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATH2TvglVdeKa1feeWVpfWjR4+W1o8cOXLGPXXKkiVLSuszZszoUSeoiy07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBcfYe+OSTT5puoaXHH3+8tH711VfXGn/btm2Vaug8tuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kIQjoncrs3u3MkiS7r777tL6unXrSuvtpmweGxsrrZedD//GG2+UvhbVRMSUExW03bLbXm17zPbOSctm237N9ofF7axONgug86azG/8jSXeesmy5pC0RcZWkLcVjAH2sbdgj4k1Jp14XaZGkNcX9NZLu7XBfADqs6m/j50bEqCRFxKjtS1o90faQpKGK6wHQIV0/ESYihiUNS3xBBzSp6qG3Q7YHJKm4Lf9KFkDjqoZ9k6QHi/sPSvpZZ9oB0C1td+NtvyTpNkkX2z4gaaWk5yT91PZDkvZL+nY3m0R1g4ODpfV2x9HbWbt2bWmdY+n9o23YI2Jpi9I3O9wLgC7i57JAEoQdSIKwA0kQdiAJwg4kwaWkzwEbN25sWVuwYEGtsZ9//vnS+lNPPVVrfPQOW3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJLSZ8FBgYGSuvvv/9+y9qcOXNKX3v48OHS+i233FJa37t3b2kdvVf5UtIAzg2EHUiCsANJEHYgCcIOJEHYgSQIO5AE57OfBdavX19ab3csvcyLL75YWuc4+rmDLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMFx9j6wcOHC0vqNN95YeezXX3+9tL5y5crKY+Ps0nbLbnu17THbOycte9r2721vL/7u6m6bAOqazm78jyTdOcXy/4yIG4q/X3S2LQCd1jbsEfGmpCM96AVAF9X5gu5h2zuK3fxZrZ5ke8j2iO2RGusCUFPVsK+SNF/SDZJGJX2/1RMjYjgiBiNisOK6AHRApbBHxKGIOBERJyX9QNLNnW0LQKdVCrvtydc2XixpZ6vnAugPbY+z235J0m2SLrZ9QNJKSbfZvkFSSNon6Ttd7PGs1+588xUrVpTWZ8yYUXnd27dvL60fO3as8tg4u7QNe0QsnWLxD7vQC4Au4ueyQBKEHUiCsANJEHYgCcIOJMEprj2wbNmy0vpNN91Ua/yNGze2rHEKKyawZQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBwRvVuZ3buV9ZEvvviitF7nFFZJmjdvXsva6OhorbFx9okIT7WcLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMH57OeA2bNnt6x9+eWXPezkdJ999lnLWrve2v3+4MILL6zUkyRddNFFpfVHH3208tjTceLEiZa1J598svS1n3/+eaV1smUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQ4zn4O2LFjR9MttLRu3bqWtXbn2s+dO7e0fv/991fqqd99+umnpfVnnnmm0rhtt+y2L7P9K9u7be+y/b1i+Wzbr9n+sLidVakDAD0xnd3445KWRcRfSvprSd+1fY2k5ZK2RMRVkrYUjwH0qbZhj4jRiHivuH9U0m5Jl0paJGlN8bQ1ku7tVpMA6jujz+y2L5f0dUnbJM2NiFFp/B8E25e0eM2QpKF6bQKoa9pht/0VSeslPRIRf7CnvKbdaSJiWNJwMUbKC04C/WBah95sz9B40H8cERuKxYdsDxT1AUlj3WkRQCe0vZS0xzfhayQdiYhHJi3/N0n/GxHP2V4uaXZEPNFmrJRb9g0bNpTWFy1a1KNOcjl+/HjL2smTJ2uNvWnTptL6yMhI5bHfeuut0vrWrVtL660uJT2d3fhbJf2DpA9sby+WrZD0nKSf2n5I0n5J357GWAAa0jbsEfE/klp9QP9mZ9sB0C38XBZIgrADSRB2IAnCDiRB2IEkmLK5DzzxROnPE2pP6Vzm2muvLa138zTS1atXl9b37dtXa/z169e3rO3Zs6fW2P2MKZuB5Ag7kARhB5Ig7EAShB1IgrADSRB2IAmOswPnGI6zA8kRdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJtw277Mtu/sr3b9i7b3yuWP23797a3F393db9dAFW1vXiF7QFJAxHxnu2vSnpX0r2S/l7SsYj492mvjItXAF3X6uIV05mffVTSaHH/qO3dki7tbHsAuu2MPrPbvlzS1yVtKxY9bHuH7dW2Z7V4zZDtEdsjtToFUMu0r0Fn+yuS3pD0TERssD1X0mFJIelfNL6r/09txmA3HuiyVrvx0wq77RmSfi5pc0T8xxT1yyX9PCL+qs04hB3ossoXnLRtST+UtHty0Isv7iYslrSzbpMAumc638Z/Q9Jbkj6QdLJYvELSUkk3aHw3fp+k7xRf5pWNxZYd6LJau/GdQtiB7uO68UByhB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSTaXnCyww5L+mTS44uLZf2oX3vr174kequqk739RatCT89nP23l9khEDDbWQIl+7a1f+5Lorape9cZuPJAEYQeSaDrsww2vv0y/9tavfUn0VlVPemv0MzuA3ml6yw6gRwg7kEQjYbd9p+3f2P7I9vImemjF9j7bHxTTUDc6P10xh96Y7Z2Tls22/ZrtD4vbKefYa6i3vpjGu2Sa8Ubfu6anP+/5Z3bb50n6raRvSTog6R1JSyPi1z1tpAXb+yQNRkTjP8Cw/TeSjkl6fmJqLdv/KulIRDxX/EM5KyKe7JPentYZTuPdpd5aTTP+j2rwvevk9OdVNLFlv1nSRxHxcUT8UdJPJC1qoI++FxFvSjpyyuJFktYU99do/H+WnmvRW1+IiNGIeK+4f1TSxDTjjb53JX31RBNhv1TS7yY9PqD+mu89JP3S9ru2h5puZgpzJ6bZKm4vabifU7WdxruXTplmvG/euyrTn9fVRNinmpqmn47/3RoRN0r6O0nfLXZXMT2rJM3X+ByAo5K+32QzxTTj6yU9EhF/aLKXyaboqyfvWxNhPyDpskmP50k62EAfU4qIg8XtmKSXNf6xo58cmphBt7gda7if/xcRhyLiRESclPQDNfjeFdOMr5f044jYUCxu/L2bqq9evW9NhP0dSVfZ/prtmZKWSNrUQB+nsX1B8cWJbF8gaYH6byrqTZIeLO4/KOlnDfbyJ/plGu9W04yr4feu8enPI6Lnf5Lu0vg38nsl/XMTPbTo6wpJ7xd/u5ruTdJLGt+t+1Lje0QPSZojaYukD4vb2X3U2wsan9p7h8aDNdBQb9/Q+EfDHZK2F393Nf3elfTVk/eNn8sCSfALOiAJwg4kQdiBJAg7kARhB5Ig7EAShB1I4v8AskwsZkLWpdIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a single image from the dataset\n",
    "image = np.array(list(test_imgs[800:1584])).reshape(28,28).astype(np.uint8)\n",
    "\n",
    "# Get the label for a specific image (Expected result = 4)\n",
    "print(\"Image label:\", int.from_bytes(test_lbls[1:2], byteorder=\"big\"))\n",
    "\n",
    "# Display plot of image\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Read in the training and test images from Keras directly.\n",
    "\n",
    "* From testing this is considerably faster than reading from the gzip files in memory, so I have chosen this method.\n",
    "\n",
    "* However when reading from the Keras directly there no header information in the first 16 bytes compared to when reading from the gzip, so it's hard to tell if you're reading it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_imgs, train_lbls), (test_imgs, test_lbls) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding the model\n",
    "\n",
    "From talking with our lecturer I found that it was best to seed the random weight generator to get reproducible results. As TensorFlow will automatically assign random weights on each run based on probibility. To achieve this I found documentation on the Keras website below, which I have adapted.\n",
    "\n",
    "https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is necessary for starting Numpy and core Python generated random numbers in a well-defined initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rn.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force TensorFlow to use single thread. Multiple threads are a potential source of non-reproducible results.\n",
    "For further details, see: https://stackoverflow.com/questions/42022950/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below tf.set_random_seed() will make random number generation in the TensorFlow backend have a well-defined initial state. For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "#### Set up a neural network model, building it layer by layer sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = kr.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Layers (Input & Hidden)\n",
    "Add a hidden layer with 1000 neurons and an input layer with 784 inputs.\n",
    "* 784 is the number of bytes per image (28 x 28)\n",
    "* A dense layers means that each neuron recieves input from all the neurons in the previous layer (all connected)\n",
    "* **linear activation function** = Takes the inputs, multiplied by the weights for each neuron, and creates an output proportional to the input. \n",
    "* **relu activation function** = (Rectified Linear Unit) All positive values stay the same and all negative values are changed to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.add(kr.layers.Dense(units=600, activation='linear', input_dim=784))\n",
    "model.add(kr.layers.Dense(units=400, activation='relu')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Layers (Output)\n",
    "\n",
    "Add a 10 neuron output layer, each output will represent a possible label from 0-9.\n",
    "* **softmax** - normalizes all outputs so must add up to 1. So the largest weighted result will be the most probable number.\n",
    "* E.g [0.1, 0.7, 0.1, 0.01...] it will choose 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(kr.layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the graph.\n",
    "* **categorical_crossentropy** - loss function that is used for single label categorization. Used classification problems where only one result can be correct. E.g number is a 9\n",
    "* **adam** - Computationally efficient and benefits from both AdaGrad and RMSProp optimizers.\n",
    "* **accuracy** - Display accuracy results when training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse image and labels into lists:\n",
    "The bitwise operator (~ tilde) can be used to invert the image. It is a complement operator. It takes one bit operand and returns its complement. If the operand is 1, it returns 0, and if it is 0, it returns 1.\n",
    "\n",
    "However I have left the image as is I.e black background with white digets, as from testing it makes prediction more accurate and the training accuracy is slighly higher.\n",
    "\n",
    "Reshape each of the 60000 images into an array list of 28 x 28 bytes (pixels). Also reshape labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.array(list(train_imgs[:])).reshape(60000, 28, 28).astype(np.uint8) / 255.0\n",
    "train_lbls = np.array(list(train_lbls[:])).astype(np.uint8)\n",
    "inputs = train_imgs.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the dataset:\n",
    "From watching the videos on Moodle relating to fine tuning models with scaling and Principal component analysis (PCA). I thought I would try it out myself to see the results, however from testing I have decided to not modify the data as it provides less accurate results unfortunately which is documented below.\n",
    "\n",
    "Preprocessing is done as sometimes a value that has a variance which is substantially larger than others. This large value will dominate others making the neural network harder to learn from, as it might miss some of the less significant values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the data:\n",
    "Scaling the data involves standardizing the values. In each column the average for the column is taken away from each value and then divided by the standard deviation. The overall standard deviation should be 1, and the mean should be 0.\n",
    "\n",
    "Code adapted from: https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale inputs \n",
    "inputs_scaled = pre.scale(inputs)\n",
    "\n",
    "# Get mean and std deviation which should be around 0 and 1\n",
    "#pd.DataFrame(inputs_scaled.mean(axis=0))\n",
    "#pd.DataFrame(inputs_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033171</td>\n",
       "      <td>-0.022463</td>\n",
       "      <td>-0.016069</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033171</td>\n",
       "      <td>-0.022463</td>\n",
       "      <td>-0.016069</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033171</td>\n",
       "      <td>-0.022463</td>\n",
       "      <td>-0.016069</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033171</td>\n",
       "      <td>-0.022463</td>\n",
       "      <td>-0.016069</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033171</td>\n",
       "      <td>-0.022463</td>\n",
       "      <td>-0.016069</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033171</td>\n",
       "      <td>-0.022463</td>\n",
       "      <td>-0.016069</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033171</td>\n",
       "      <td>-0.022463</td>\n",
       "      <td>-0.016069</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033171</td>\n",
       "      <td>-0.022463</td>\n",
       "      <td>-0.016069</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033171</td>\n",
       "      <td>-0.022463</td>\n",
       "      <td>-0.016069</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033171</td>\n",
       "      <td>-0.022463</td>\n",
       "      <td>-0.016069</td>\n",
       "      <td>-0.011432</td>\n",
       "      <td>-0.009007</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...       774  \\\n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.033171   \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.033171   \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.033171   \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.033171   \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.033171   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   \n",
       "59995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.033171   \n",
       "59996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.033171   \n",
       "59997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.033171   \n",
       "59998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.033171   \n",
       "59999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ... -0.033171   \n",
       "\n",
       "            775       776       777       778      779  780  781  782  783  \n",
       "0     -0.022463 -0.016069 -0.011432 -0.009007 -0.00577  0.0  0.0  0.0  0.0  \n",
       "1     -0.022463 -0.016069 -0.011432 -0.009007 -0.00577  0.0  0.0  0.0  0.0  \n",
       "2     -0.022463 -0.016069 -0.011432 -0.009007 -0.00577  0.0  0.0  0.0  0.0  \n",
       "3     -0.022463 -0.016069 -0.011432 -0.009007 -0.00577  0.0  0.0  0.0  0.0  \n",
       "4     -0.022463 -0.016069 -0.011432 -0.009007 -0.00577  0.0  0.0  0.0  0.0  \n",
       "...         ...       ...       ...       ...      ...  ...  ...  ...  ...  \n",
       "59995 -0.022463 -0.016069 -0.011432 -0.009007 -0.00577  0.0  0.0  0.0  0.0  \n",
       "59996 -0.022463 -0.016069 -0.011432 -0.009007 -0.00577  0.0  0.0  0.0  0.0  \n",
       "59997 -0.022463 -0.016069 -0.011432 -0.009007 -0.00577  0.0  0.0  0.0  0.0  \n",
       "59998 -0.022463 -0.016069 -0.011432 -0.009007 -0.00577  0.0  0.0  0.0  0.0  \n",
       "59999 -0.022463 -0.016069 -0.011432 -0.009007 -0.00577  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[60000 rows x 784 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display scaled array using Pandas\n",
    "pd.DataFrame(inputs_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal component analysis (PCA) & Whitening:\n",
    "\n",
    "PCA involves transforming a number of correlated variables into a smaller uncorrelated variables which are called principal components.\n",
    "\n",
    "Whitening involves multiplying each component by the square root and then dividing by the singular values which ensures uncorrelated outputs. Whitening will remove some information from the inputs, but can sometimes improve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.214854</td>\n",
       "      <td>-0.633921</td>\n",
       "      <td>-0.053303</td>\n",
       "      <td>-1.292893</td>\n",
       "      <td>-0.066811</td>\n",
       "      <td>-0.604610</td>\n",
       "      <td>0.698640</td>\n",
       "      <td>0.508199</td>\n",
       "      <td>-1.181288</td>\n",
       "      <td>0.698025</td>\n",
       "      <td>...</td>\n",
       "      <td>1.934448e-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.321313</td>\n",
       "      <td>14.340837</td>\n",
       "      <td>18.642379</td>\n",
       "      <td>0.349014</td>\n",
       "      <td>-3.397805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.753950</td>\n",
       "      <td>-0.597798</td>\n",
       "      <td>1.296674</td>\n",
       "      <td>-1.071908</td>\n",
       "      <td>-2.024822</td>\n",
       "      <td>-0.473206</td>\n",
       "      <td>-0.134418</td>\n",
       "      <td>-0.333827</td>\n",
       "      <td>0.131477</td>\n",
       "      <td>0.531074</td>\n",
       "      <td>...</td>\n",
       "      <td>1.749181e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.037058</td>\n",
       "      <td>1.255922</td>\n",
       "      <td>6.270980</td>\n",
       "      <td>-4.164999</td>\n",
       "      <td>17.605331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.089888</td>\n",
       "      <td>0.795099</td>\n",
       "      <td>-0.409894</td>\n",
       "      <td>1.212060</td>\n",
       "      <td>-0.750626</td>\n",
       "      <td>-0.004773</td>\n",
       "      <td>-2.564855</td>\n",
       "      <td>1.172206</td>\n",
       "      <td>-0.372232</td>\n",
       "      <td>-0.627951</td>\n",
       "      <td>...</td>\n",
       "      <td>3.424522e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.018421</td>\n",
       "      <td>-10.340331</td>\n",
       "      <td>-8.907293</td>\n",
       "      <td>13.307287</td>\n",
       "      <td>5.379773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-1.385394</td>\n",
       "      <td>-1.231043</td>\n",
       "      <td>0.595024</td>\n",
       "      <td>0.246311</td>\n",
       "      <td>-0.004536</td>\n",
       "      <td>1.819619</td>\n",
       "      <td>-1.414364</td>\n",
       "      <td>-0.214098</td>\n",
       "      <td>0.983764</td>\n",
       "      <td>0.039087</td>\n",
       "      <td>...</td>\n",
       "      <td>1.606204e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044644</td>\n",
       "      <td>-3.454146</td>\n",
       "      <td>4.230403</td>\n",
       "      <td>6.892552</td>\n",
       "      <td>-4.518633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.663557</td>\n",
       "      <td>1.481116</td>\n",
       "      <td>0.035559</td>\n",
       "      <td>-0.562275</td>\n",
       "      <td>0.240258</td>\n",
       "      <td>0.112429</td>\n",
       "      <td>-0.273689</td>\n",
       "      <td>-1.289756</td>\n",
       "      <td>0.732797</td>\n",
       "      <td>0.366243</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.176150e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.024908</td>\n",
       "      <td>-1.391055</td>\n",
       "      <td>2.147943</td>\n",
       "      <td>3.802038</td>\n",
       "      <td>3.453613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59995</td>\n",
       "      <td>-0.172708</td>\n",
       "      <td>-0.594998</td>\n",
       "      <td>0.295587</td>\n",
       "      <td>-0.807536</td>\n",
       "      <td>0.744740</td>\n",
       "      <td>0.616359</td>\n",
       "      <td>-1.488535</td>\n",
       "      <td>-0.755669</td>\n",
       "      <td>-1.608234</td>\n",
       "      <td>1.126865</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.141710e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.241807</td>\n",
       "      <td>0.935214</td>\n",
       "      <td>-1.893164</td>\n",
       "      <td>-10.826239</td>\n",
       "      <td>9.247554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59996</td>\n",
       "      <td>0.264826</td>\n",
       "      <td>-1.394660</td>\n",
       "      <td>-0.193374</td>\n",
       "      <td>-1.512166</td>\n",
       "      <td>0.485618</td>\n",
       "      <td>0.480543</td>\n",
       "      <td>-1.454452</td>\n",
       "      <td>1.607075</td>\n",
       "      <td>-2.173570</td>\n",
       "      <td>0.095414</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.413602e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104544</td>\n",
       "      <td>1.074031</td>\n",
       "      <td>-0.910176</td>\n",
       "      <td>4.538898</td>\n",
       "      <td>4.995267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59997</td>\n",
       "      <td>-0.308680</td>\n",
       "      <td>0.324545</td>\n",
       "      <td>-0.560151</td>\n",
       "      <td>-1.662534</td>\n",
       "      <td>-0.514763</td>\n",
       "      <td>-1.202745</td>\n",
       "      <td>-1.386961</td>\n",
       "      <td>-0.587777</td>\n",
       "      <td>-0.284422</td>\n",
       "      <td>0.943168</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.211944e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.261956</td>\n",
       "      <td>3.507706</td>\n",
       "      <td>1.078574</td>\n",
       "      <td>1.789468</td>\n",
       "      <td>17.024285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59998</td>\n",
       "      <td>0.226423</td>\n",
       "      <td>-0.011337</td>\n",
       "      <td>1.117329</td>\n",
       "      <td>0.797252</td>\n",
       "      <td>-1.275595</td>\n",
       "      <td>-1.405840</td>\n",
       "      <td>-0.320593</td>\n",
       "      <td>0.447549</td>\n",
       "      <td>-0.053605</td>\n",
       "      <td>-0.330114</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.671874e-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029328</td>\n",
       "      <td>-9.675645</td>\n",
       "      <td>2.207996</td>\n",
       "      <td>4.522283</td>\n",
       "      <td>-5.258369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59999</td>\n",
       "      <td>-0.300675</td>\n",
       "      <td>-0.050115</td>\n",
       "      <td>1.209001</td>\n",
       "      <td>-0.281168</td>\n",
       "      <td>-0.484622</td>\n",
       "      <td>0.606041</td>\n",
       "      <td>-1.377720</td>\n",
       "      <td>-0.491203</td>\n",
       "      <td>0.283704</td>\n",
       "      <td>-0.424876</td>\n",
       "      <td>...</td>\n",
       "      <td>8.226771e-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000711</td>\n",
       "      <td>1.698774</td>\n",
       "      <td>6.259551</td>\n",
       "      <td>-1.136778</td>\n",
       "      <td>-4.207936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.214854 -0.633921 -0.053303 -1.292893 -0.066811 -0.604610  0.698640   \n",
       "1      1.753950 -0.597798  1.296674 -1.071908 -2.024822 -0.473206 -0.134418   \n",
       "2     -0.089888  0.795099 -0.409894  1.212060 -0.750626 -0.004773 -2.564855   \n",
       "3     -1.385394 -1.231043  0.595024  0.246311 -0.004536  1.819619 -1.414364   \n",
       "4     -0.663557  1.481116  0.035559 -0.562275  0.240258  0.112429 -0.273689   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "59995 -0.172708 -0.594998  0.295587 -0.807536  0.744740  0.616359 -1.488535   \n",
       "59996  0.264826 -1.394660 -0.193374 -1.512166  0.485618  0.480543 -1.454452   \n",
       "59997 -0.308680  0.324545 -0.560151 -1.662534 -0.514763 -1.202745 -1.386961   \n",
       "59998  0.226423 -0.011337  1.117329  0.797252 -1.275595 -1.405840 -0.320593   \n",
       "59999 -0.300675 -0.050115  1.209001 -0.281168 -0.484622  0.606041 -1.377720   \n",
       "\n",
       "            7         8         9    ...           774  775  776  777  778  \\\n",
       "0      0.508199 -1.181288  0.698025  ...  1.934448e-15  0.0  0.0  0.0  0.0   \n",
       "1     -0.333827  0.131477  0.531074  ...  1.749181e-06  0.0  0.0  0.0  0.0   \n",
       "2      1.172206 -0.372232 -0.627951  ...  3.424522e-05  0.0  0.0  0.0  0.0   \n",
       "3     -0.214098  0.983764  0.039087  ...  1.606204e-06  0.0  0.0  0.0  0.0   \n",
       "4     -1.289756  0.732797  0.366243  ... -2.176150e-05  0.0  0.0  0.0  0.0   \n",
       "...         ...       ...       ...  ...           ...  ...  ...  ...  ...   \n",
       "59995 -0.755669 -1.608234  1.126865  ... -5.141710e-06  0.0  0.0  0.0  0.0   \n",
       "59996  1.607075 -2.173570  0.095414  ... -7.413602e-06  0.0  0.0  0.0  0.0   \n",
       "59997 -0.587777 -0.284422  0.943168  ... -1.211944e-07  0.0  0.0  0.0  0.0   \n",
       "59998  0.447549 -0.053605 -0.330114  ... -7.671874e-06  0.0  0.0  0.0  0.0   \n",
       "59999 -0.491203  0.283704 -0.424876  ...  8.226771e-08  0.0  0.0  0.0  0.0   \n",
       "\n",
       "            779        780        781        782        783  \n",
       "0     -4.321313  14.340837  18.642379   0.349014  -3.397805  \n",
       "1     -0.037058   1.255922   6.270980  -4.164999  17.605331  \n",
       "2     -0.018421 -10.340331  -8.907293  13.307287   5.379773  \n",
       "3      0.044644  -3.454146   4.230403   6.892552  -4.518633  \n",
       "4     -0.024908  -1.391055   2.147943   3.802038   3.453613  \n",
       "...         ...        ...        ...        ...        ...  \n",
       "59995 -0.241807   0.935214  -1.893164 -10.826239   9.247554  \n",
       "59996  0.104544   1.074031  -0.910176   4.538898   4.995267  \n",
       "59997 -0.261956   3.507706   1.078574   1.789468  17.024285  \n",
       "59998  0.029328  -9.675645   2.207996   4.522283  -5.258369  \n",
       "59999 -0.000711   1.698774   6.259551  -1.136778  -4.207936  \n",
       "\n",
       "[60000 rows x 784 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whiten using sklearn.decomposition\n",
    "pca = dec.PCA(whiten=True)\n",
    "pca.fit(inputs)\n",
    "\n",
    "# Covert to pandas dataframe and display.\n",
    "inputs_white = pd.DataFrame(pca.transform(inputs))\n",
    "inputs_white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of Preprocessing image data:\n",
    "**Scaling** - Training accuracy was quite high with a 97% accuracy but correct results only deemed 907 correct results out of 10000.\n",
    "\n",
    "**PCA & Whitening** - Training accuracy was again quite high with a slighly higher accuracy of 98%. However once again the correct results only deemed 1204 correct results out of 10000, which is far too low.\n",
    "\n",
    "**Conclusion** - From testing both methods multiple times the results didn't change much, so maybe too much information is being changed to give accurate results. When nothing is done to the dataset it gets on average 9700 correct which is what I will use. However it was a good learning experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode labels:\n",
    "Encode into binary using sklearn, so they can be compared easily.\n",
    "E.g 0 = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 1 = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = pre.LabelBinarizer()\n",
    "encoder.fit(train_lbls)\n",
    "outputs = encoder.transform(train_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lbls[0], outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out binary value for each number 0-9. \n",
    "As you can see each value from 0-9, with the 1 being the correct number index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[1 0 0 0 0 0 0 0 0 0]]\n",
      "1 [[0 1 0 0 0 0 0 0 0 0]]\n",
      "2 [[0 0 1 0 0 0 0 0 0 0]]\n",
      "3 [[0 0 0 1 0 0 0 0 0 0]]\n",
      "4 [[0 0 0 0 1 0 0 0 0 0]]\n",
      "5 [[0 0 0 0 0 1 0 0 0 0]]\n",
      "6 [[0 0 0 0 0 0 1 0 0 0]]\n",
      "7 [[0 0 0 0 0 0 0 1 0 0]]\n",
      "8 [[0 0 0 0 0 0 0 0 1 0]]\n",
      "9 [[0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i, encoder.transform([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and save the Model\n",
    "Rather than retraining the model again and again I wanted to be able to save and load it, so I researched and found a way to achieve this, by saving a binary .h5 model file. This could then be loaded into the web application for quick prediction.\n",
    "\n",
    "Code adapted from: https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\n",
    "\n",
    "**One Epoch** - Is when an entire dataset is passed forward and backward through the neural network only once.\n",
    "I have decided to train the model using 10 epochs, which from testing it seems to peak at. This can be seen in the graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.2436 - acc: 0.9277\n",
      "Epoch 2/10\n",
      " - 4s - loss: 0.0998 - acc: 0.9698\n",
      "Epoch 3/10\n",
      " - 4s - loss: 0.0685 - acc: 0.9783\n",
      "Epoch 4/10\n",
      " - 4s - loss: 0.0544 - acc: 0.9826\n",
      "Epoch 5/10\n",
      " - 4s - loss: 0.0478 - acc: 0.9843\n",
      "Epoch 6/10\n",
      " - 4s - loss: 0.0376 - acc: 0.9877\n",
      "Epoch 7/10\n",
      " - 4s - loss: 0.0342 - acc: 0.9889\n",
      "Epoch 8/10\n",
      " - 4s - loss: 0.0301 - acc: 0.9898\n",
      "Epoch 9/10\n",
      " - 4s - loss: 0.0309 - acc: 0.9896\n",
      "Epoch 10/10\n",
      " - 4s - loss: 0.0234 - acc: 0.9921\n"
     ]
    }
   ],
   "source": [
    "model_log = model.fit(inputs, outputs, verbose=2, epochs=10, batch_size=200)\n",
    "model.save('trained_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display accuracy and loss results on a graph\n",
    "Code adapted from: https://towardsdatascience.com/a-simple-2d-cnn-for-mnist-digit-recognition-a998dbc1e79a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the metrics for accuracy and loss.\n",
    "fig = plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(model_log.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(model_log.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained model\n",
    "Parse images and labels into lists, and reshape like before. This time using the 10000 training images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = np.array(list(test_imgs[:])).reshape(10000, 784).astype(np.uint8) / 255.0\n",
    "test_lbls = np.array(list(test_lbls[:])).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the sum of correct values \n",
    "Calculate the sum of correct results out of all 10000 images using the labels\n",
    "\n",
    "The last test returned a result of 9771, so it's about 97% of the time correct which correlates well with the accuracy results in the graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sum of correct results: 9781\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSum of correct results:\", (encoder.inverse_transform(model.predict(test_imgs)) == test_lbls).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict image using model\n",
    "Pick an image from the test images, and call predict. ([11:12], is the number 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test image: [[1.2635444e-08 2.0018479e-19 5.1486220e-15 1.6143477e-16 3.0222049e-13\n",
      "  2.2541767e-10 1.0000000e+00 2.2807827e-14 1.0179507e-08 3.1492902e-18]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest image:\", model.predict(test_imgs[11:12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From testing the model.predict it would return an array of possibilities. The highest value would be the right answer.\n",
    "To get the largest value I looked how to get the index of the highest value in a numpy array and found argmax() which does this nicely.\n",
    "\n",
    "Code adapted from: https://kite.com/python/examples/5750/numpy-find-the-index-of-the-largest-element-of-an-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResults:\", model.predict(test_imgs[11:12]).argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display correct image \n",
    "If it's the same as the predicted above then the prediction is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_imgs[11].reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
